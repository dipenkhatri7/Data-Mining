{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the tools\n",
    "\n",
    "- pandas for data analysis.\n",
    "- NumPy for numerical operations.\n",
    "- Matplotlib/seaborn for plotting or data visualization.\n",
    "- Scikit-Learn for machine learning modelling and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular EDA and plotting libraries\n",
    "import numpy as np # np is short for numpy\n",
    "\n",
    "import pandas as pd # pandas is so commonly used, it's shortened to pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns # seaborn gets shortened to sns, TK - can seaborn be removed for matplotlib (simpler)?\n",
    "\n",
    "## Models\n",
    "import sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier # For ANN\n",
    "from sklearn.svm import SVC  # For SVM (Support Vector Machine)\n",
    "from sklearn.naive_bayes import GaussianNB  # For Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB  # For binary classification\n",
    "\n",
    "\n",
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.metrics import plot_roc_curve # note: this was changed in Scikit-Learn 1.2+ to be \"RocCurveDisplay\" (see below)\n",
    "from sklearn.metrics import RocCurveDisplay # new in Scikit-Learn 1.2+\n",
    "\n",
    "# Print last updated\n",
    "import datetime\n",
    "print(f\"Notebook last updated: {datetime.datetime.now()}\\n\")\n",
    "\n",
    "# Print versions of libraries we're using (as long as yours are equal or greater than these, your code should work)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-Learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get project root directory\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from src import config\n",
    "\n",
    "# Use absolute path instead of relative\n",
    "df = pd.read_csv(os.path.join(project_dir, config.HEART_DATA_PATH))\n",
    "df.shape # (rows,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Exploration (exploratory data analysis or EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head of our DataFrame \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the top 10\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of positive(1) and negative(0) samples in pur dataframe\n",
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized value counts\n",
    "df.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the value counts with a bar graph\n",
    "df.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Comparing one feature to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sex, 1=male, 0=female \n",
    "df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare target column with sex column\n",
    "pd.crosstab(index=df.target, columns=df.sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Making our comparison visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a plot\n",
    "pd.crosstab(df.target, df.sex).plot(kind=\"bar\", figsize=(10,6), color=[\"salmon\", \"lightblue\"])\n",
    "\n",
    "# Add some attributes to it\n",
    "plt.title(\"Heart Disease Frequency vs Sex\")\n",
    "plt.xlabel(\"0 = No Disease, 1 = Disease\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend([\"Female\", \"Male\"])\n",
    "plt.xticks(rotation=0); # keep the labels on the x-axis vertical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparing age and maximum heart rate\n",
    "Let's combine a couple of independent variables, such as, age and thalach (maximum heart rate) and then comparing them to our target variable heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another figure\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Start with positve examples\n",
    "plt.scatter(df.age[df.target==1], \n",
    "            df.thalach[df.target==1], \n",
    "            c=\"salmon\") # define it as a scatter figure\n",
    "\n",
    "# Now for negative examples, we want them on the same plot, so we call plt again\n",
    "plt.scatter(df.age[df.target==0], \n",
    "            df.thalach[df.target==0], \n",
    "            c=\"lightblue\") # axis always come as (x, y)\n",
    "\n",
    "# Add some helpful info\n",
    "plt.title(\"Heart Disease in function of Age and Max Heart Rate\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.legend([\"Disease\", \"No Disease\"])\n",
    "plt.ylabel(\"Max Heart Rate\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram to check the distribution of the variable age\n",
    "df.age.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cp.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.trestbps.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.chol.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fbs.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.restecg.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.thalach.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.exang.plot.hist(edgecolor='white', color='#4878CF'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.oldpeak.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.slope.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ca.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.thal.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.plot.hist(edgecolor='white', color='#4878CF');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparing heart disease frequency and chest pain type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=df.cp, columns=df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a new crosstab and base plot\n",
    "pd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n",
    "                                   figsize=(10,6), \n",
    "                                   color=[\"lightblue\", \"salmon\"])\n",
    "\n",
    "# Add attributes to the plot to make it more readable\n",
    "plt.title(\"Heart Disease Frequency Per Chest Pain Type\")\n",
    "plt.xlabel(\"Chest Pain Type\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend([\"No Disease\", \"Disease\"])\n",
    "plt.xticks(rotation = 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Correlation between independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between our independent variables\n",
    "corr_matrix = df.corr()\n",
    "corr_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the correlation\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True, \n",
    "            linewidths=0.5, \n",
    "            fmt= \".2f\", \n",
    "            cmap=\"YlGnBu\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling\n",
    "We will now be predicting our target variable variable using all of the other variables. For this, we will split the target variable from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything except target variable\n",
    "X = df.drop(labels=\"target\", axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = df.target.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables (no target column)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets (in the form of a NumPy array)\n",
    "y, type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating a training and test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will split our data into a training set and a test set. To split our data into a training and test set, we can use Scikit-Learn's sklearn.model_selection.train_test_split() and feed it our independent and dependent variables (X & y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility (since train_test_split is random by default, setting the seed will create reproducible splits)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into train & test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n",
    "                                                    y, # dependent variable\n",
    "                                                    test_size = 0.2) # percentage of data to use for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data of independent variable\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data of dependent variable\n",
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data of independent variable\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data of dependent variable\n",
    "y_test, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Choosing a model\n",
    "We'll start by trying the following models and comparing their results.\n",
    "\n",
    "1. Logistic Regression - sklearn.linear_model.LogisticRegression()\n",
    "2. K-Nearest Neighbors - sklearn.neighbors.KNeighboursClassifier()\n",
    "3. RandomForest - sklearn.ensemble.RandomForestClassifier()\n",
    "4. Decision Tree: sklearn.tree.DecisionTreeClassifier()\n",
    "5. SVC:sklearn.svm.\n",
    "6. ANN: MLPClassifier(max_iter=1000)\n",
    "7. Naive Bayes (Gaussian): sklearn.naive_bayes.GaussianNB()\n",
    "8. Naive Bayes (Bernoulli): sklearn.naive_bayes.BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put models in a dictionary\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(max_iter=100), # Note: if you see a warning about \"convergence not reached\", you can increase `max_iter` until convergence is reached\n",
    "          \"Random Forest\": RandomForestClassifier(),\n",
    "          \"Decision Tree\": DecisionTreeClassifier(),\n",
    "          \"SVC\": SVC(),\n",
    "          \"ANN\": MLPClassifier(max_iter=1000),\n",
    "          \"Naive Bayes (Gaussian)\": GaussianNB(),\n",
    "          \"Naive Bayes (Bernoulli)\": BernoulliNB()}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Fits and evaluates given machine learning models.\n",
    "    # models : a dict of different Scikit-Learn machine learning models\n",
    "    # X_train : training data\n",
    "    # X_test : testing data\n",
    "    # y_train : labels assosciated with training data\n",
    "    # y_test : labels assosciated with test data\n",
    "    \n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = fit_and_score(models=models, \n",
    "                             X_train=X_train, \n",
    "                             X_test=X_test, \n",
    "                             y_train=y_train, \n",
    "                             y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores = {\n",
    "    name: recall_score(y_test, model.predict(X_test), average='weighted')\n",
    "    for name, model in models.items()\n",
    "}\n",
    "\n",
    "recall_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing the results of several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n",
    "model_compare.T.plot.bar();\n",
    "\n",
    "model_compare = pd.DataFrame(recall_scores, index=['Recall'])\n",
    "model_compare.T.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more exaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "precision_scores = {\n",
    "    name: precision_score(y_test, model.predict(X_test), average='weighted')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {\n",
    "    name: f1_score(y_test, model.predict(X_test), average='weighted')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_scores = {}\n",
    "for name, model in models.items():\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    tn = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    specificity_scores[name] = specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error_scores = {\n",
    "    name: 1 - accuracy\n",
    "    for name, accuracy in model_scores.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame({\n",
    "    \"Accuracy (%)\": [score * 100 for score in model_scores.values()],\n",
    "    \"Recall (%)\": [score * 100 for score in recall_scores.values()],\n",
    "     \"Precision (%)\": [score * 100 for score in precision_scores.values()],\n",
    "    \"F1 Score (%)\": [score * 100 for score in f1_scores.values()],\n",
    "    \"Specificity (%)\": [score * 100 for score in specificity_scores.values()],\n",
    "    \"Classification Error (%)\": [error * 100 for error in classification_error_scores.values()]\n",
    "    \n",
    "}, index=model_scores.keys())\n",
    "\n",
    "# Display the DataFrame\n",
    "print(model_comparison)\n",
    "\n",
    "# Optional: Pretty-print the table\n",
    "model_comparison.style.format({\"Accuracy (%)\": \"{:.2f}\", \n",
    "                               \"Recall (%)\": \"{:.2f}\",\n",
    "                               \"Precision (%)\": \"{:.2f}\", \n",
    "                               \"F1 Score (%)\": \"{:.2f}\", \n",
    "                               \"Specificity (%)\": \"{:.2f}\", \n",
    "                               \"Classification Error (%)\": \"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameter tuning and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tune KNeighborsClassifier (K-Nearest Neighbors or KNN) by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of train scores\n",
    "train_scores = []\n",
    "\n",
    "# Create a list of test scores\n",
    "test_scores = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21) # 1 to 20\n",
    "\n",
    "# Setup algorithm\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different neighbors values\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors = i) # set neighbors value\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Update the training scores\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    \n",
    "    # Update the test scores\n",
    "    test_scores.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN's train scores\n",
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize KNN score test and train data\n",
    "plt.plot(neighbors, train_scores, label=\"Train score\")\n",
    "plt.plot(neighbors, test_scores, label=\"Test score\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tuning models with with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LogisticRegression hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Different RandomForestClassifier hyperparameters\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n",
    "           \"max_depth\": [None, 3, 5, 10],\n",
    "           \"min_samples_split\": np.arange(2, 20, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2)}\n",
    "\n",
    "# Define DecisionTreeClassifier hyperparameter\n",
    "dt_grid = {\n",
    "    \"max_depth\": [None, 3, 5, 10, 20],\n",
    "    \"min_samples_split\": np.arange(2, 20, 2),\n",
    "    \"min_samples_leaf\": np.arange(1, 20, 2),\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# Define SVC hyperparameters\n",
    "svc_grid = {\n",
    "    \"C\": np.logspace(-4, 4, 20),\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# Different KNN hyperparameters\n",
    "knn_grid = {\n",
    "    \"n_neighbors\": np.arange(1, 21),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": np.arange(20, 60, 5),\n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "\n",
    "# Define ANN hyperparameters\n",
    "ann_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50, 50), (100, 100)],  # Different layer sizes\n",
    "    \"activation\": [\"tanh\", \"relu\"],  # Activation functions\n",
    "    \"solver\": [\"adam\", \"sgd\"],  # Optimization solvers\n",
    "    \"alpha\": np.logspace(-4, 4, 20),  # Regularization term\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"],  # Learning rate strategy\n",
    "    \"max_iter\": [500, 1000]  # Max iterations for convergence\n",
    "}\n",
    "\n",
    "# Define Naive Bayes (Gaussian) hyperparameters\n",
    "nb_gaussian_grid = {\n",
    "    \"var_smoothing\": np.logspace(-9, 0, 10)  # Smoothing parameter\n",
    "}\n",
    "\n",
    "# Define Naive Bayes (Bernoulli) hyperparameters\n",
    "nb_bernoulli_grid = {\n",
    "    \"alpha\": np.logspace(-4, 4, 20),  # Additive smoothing parameter\n",
    "    \"binarize\": [0.0, 0.1, 0.2, 0.3],  # Threshold for binarizing input\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=20,\n",
    "                           verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters\n",
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the randomized search random forest model\n",
    "rs_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for DecisionTreeClassifier\n",
    "rs_dt = RandomizedSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    param_distributions=dt_grid,\n",
    "    cv=5,\n",
    "    n_iter=20,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random hyperparameter search model\n",
    "rs_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters\n",
    "rs_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the randomized search Decision Tree model\n",
    "rs_dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# # Setup random seed\n",
    "# np.random.seed(42)\n",
    "\n",
    "# # Setup random hyperparameter search for SVC\n",
    "# rs_svc = RandomizedSearchCV(SVC(),\n",
    "#                             param_distributions=svc_grid,\n",
    "#                             cv=5,\n",
    "#                             n_iter=20,\n",
    "#                             verbose=True)\n",
    "\n",
    "# # Fit random hyperparameter search model\n",
    "# rs_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the best parameters\n",
    "# rs_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the randomized search SVC model\n",
    "# rs_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup random hyperparameter search for KNN\n",
    "rs_knn = RandomizedSearchCV(KNeighborsClassifier(),\n",
    "                            param_distributions=knn_grid,\n",
    "                            cv=5,\n",
    "                            n_iter=20,\n",
    "                            verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters\n",
    "rs_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the randomized search KNN model\n",
    "rs_knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Setup random hyperparameter search for GaussianNB\n",
    "rs_nb_gaussian = RandomizedSearchCV(GaussianNB(), \n",
    "                                    param_distributions=nb_gaussian_grid, \n",
    "                                    cv=5, \n",
    "                                    n_iter=20, \n",
    "                                    verbose=True, \n",
    "                                    random_state=42)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_nb_gaussian.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters for Naive Bayes (Gaussian)\n",
    "print(\"Best parameters for Gaussian Naive Bayes:\", rs_nb_gaussian.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Naive Bayes (Gaussian) model test score:\", rs_nb_gaussian.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Setup random hyperparameter search for BernoulliNB\n",
    "rs_nb_bernoulli = RandomizedSearchCV(BernoulliNB(), \n",
    "                                     param_distributions=nb_bernoulli_grid, \n",
    "                                     cv=5, \n",
    "                                     n_iter=20, \n",
    "                                     verbose=True, \n",
    "                                     random_state=42)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_nb_bernoulli.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters for Naive Bayes (Bernoulli)\n",
    "print(\"Best parameters for Bernoulli Naive Bayes:\", rs_nb_bernoulli.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "print(\"Naive Bayes (Bernoulli) model test score:\", rs_nb_bernoulli.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate classification error and specificity\n",
    "def classification_error(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum(axis=1) - cm.diagonal()  # True negatives for each class\n",
    "    fp = cm.sum(axis=0) - cm.diagonal()  # False positives for each class\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)  # Avoid division by zero\n",
    "    return specificity_per_class.mean()  # Average specificity across all classes\n",
    "\n",
    "tuned_model_scores = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_log_reg.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_log_reg.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_log_reg.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_knn.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_knn.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_knn.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_knn.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_rf.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_rf.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_rf.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_dt.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_dt.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_dt.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (GaussianNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_nb_gaussian.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_nb_gaussian.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_nb_gaussian.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (BernoulliNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_nb_bernoulli.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_nb_bernoulli.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_nb_bernoulli.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "tuned_model_comparison = pd.DataFrame(tuned_model_scores).T * 100  # Convert to percentages\n",
    "\n",
    "# Display the DataFrame\n",
    "print(tuned_model_comparison)\n",
    "\n",
    "# Optional: Pretty-print the table\n",
    "tuned_model_comparison.style.format({\"Accuracy\": \"{:.2f}%\",\n",
    "    \"Precision\": \"{:.2f}%\",\n",
    "    \"Recall (Sensitivity)\": \"{:.2f}%\",\n",
    "    \"Classification Error\": \"{:.2f}%\",\n",
    "    \"Specificity\": \"{:.2f}%\",\n",
    "    \"F1 Score\": \"{:.2f}%\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning K-Nearest Neighbors (KNN) with VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define other classifiers for VotingClassifier\n",
    "log_reg = LogisticRegression(max_iter=100)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "svc = SVC()\n",
    "\n",
    "# Setup VotingClassifier with KNN\n",
    "voting_knn = VotingClassifier(estimators=[('log_reg', log_reg), ('rf', rf), ('svc', svc)], voting='hard')\n",
    "\n",
    "# Setup random hyperparameter search for VotingClassifier\n",
    "rs_voting_knn = RandomizedSearchCV(voting_knn,\n",
    "                                   param_distributions={'log_reg__C': np.logspace(-4, 4, 20),\n",
    "                                                        'svc__C': np.logspace(-4, 4, 20),\n",
    "                                                        'rf__max_depth': [None, 5, 10, 15],\n",
    "                                                        'rf__min_samples_split': np.arange(2, 20, 2)},\n",
    "                                   cv=5,\n",
    "                                   n_iter=20,\n",
    "                                   verbose=True,\n",
    "                                   random_state=42)\n",
    "\n",
    "# Fit the RandomizedSearchCV model\n",
    "rs_voting_knn.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model score\n",
    "print(\"Best parameters for VotingClassifier with KNN:\", rs_voting_knn.best_params_)\n",
    "print(\"VotingClassifier (KNN) model test score:\", rs_voting_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning DecisionTreeClassifier with RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for RandomForestClassifier (which can also be used for DecisionTreeClassifier)\n",
    "dt_grid = {\n",
    "    \"max_depth\": [None, 3, 5, 10, 20],  # Vary depth of the tree\n",
    "    \"min_samples_split\": np.arange(2, 20, 2),  # Minimum samples required to split a node\n",
    "    \"min_samples_leaf\": np.arange(1, 20, 2),  # Minimum samples required to be at a leaf node\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],  # Features to consider for best split\n",
    "    \"criterion\": [\"gini\", \"entropy\"],  # Split quality criteria\n",
    "}\n",
    "\n",
    "rs_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=dt_grid, n_iter=20, cv=5, verbose=True, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rs_dt.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters for DecisionTreeClassifier\n",
    "print(\"Best parameters for DecisionTreeClassifier:\", rs_dt.best_params_)\n",
    "\n",
    "# Evaluate the tuned DecisionTreeClassifier\n",
    "print(\"DecisionTreeClassifier test score:\", rs_dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tuning Logistic Regression with VotingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers for VotingClassifier\n",
    "log_reg = LogisticRegression(max_iter=100)\n",
    "svc = SVC()\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Setup VotingClassifier with Logistic Regression\n",
    "voting_log_reg = VotingClassifier(estimators=[('svc', svc), ('rf', rf)], voting='hard')\n",
    "\n",
    "# Setup random hyperparameter search for VotingClassifier with Logistic Regression\n",
    "rs_voting_log_reg = RandomizedSearchCV(voting_log_reg,\n",
    "                                      param_distributions={'svc__C': np.logspace(-4, 4, 20),\n",
    "                                                           'rf__max_depth': [None, 5, 10],\n",
    "                                                           'rf__min_samples_split': np.arange(2, 20, 2)},\n",
    "                                      cv=5,\n",
    "                                      n_iter=20,\n",
    "                                      verbose=True,\n",
    "                                      random_state=42)\n",
    "\n",
    "# Fit the RandomizedSearchCV model\n",
    "rs_voting_log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model score\n",
    "print(\"Best parameters for VotingClassifier with Logistic Regression:\", rs_voting_log_reg.best_params_)\n",
    "print(\"VotingClassifier (Logistic Regression) model test score:\", rs_voting_log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Support Vector Classifier (SVC) with VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers for VotingClassifier\n",
    "log_reg = LogisticRegression(max_iter=100)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "svc = SVC()\n",
    "\n",
    "# Setup VotingClassifier with SVC\n",
    "voting_svc = VotingClassifier(estimators=[('log_reg', log_reg), ('rf', rf)], voting='hard')\n",
    "\n",
    "# Setup random hyperparameter search for VotingClassifier with SVC\n",
    "rs_voting_svc = RandomizedSearchCV(voting_svc,\n",
    "                                   param_distributions={'log_reg__C': np.logspace(-4, 4, 20),\n",
    "                                                        'rf__max_depth': [None, 5, 10],\n",
    "                                                        'rf__min_samples_split': np.arange(2, 20, 2)},\n",
    "                                   cv=5,\n",
    "                                   n_iter=20,\n",
    "                                   verbose=True,\n",
    "                                   random_state=42)\n",
    "\n",
    "# Fit the RandomizedSearchCV model\n",
    "rs_voting_svc.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model score\n",
    "print(\"Best parameters for VotingClassifier with SVC:\", rs_voting_svc.best_params_)\n",
    "print(\"VotingClassifier (SVC) model test score:\", rs_voting_svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Tuning SVC with BaggingClassifier -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define hyperparameters for SVC\n",
    "# svc_grid = {\n",
    "#     \"base_estimator__C\": np.logspace(-4, 4, 20),  # C parameter for SVC\n",
    "#     \"base_estimator__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],  # Kernel for SVC\n",
    "#     \"base_estimator__gamma\": [\"scale\", \"auto\"],  # Gamma for SVC\n",
    "#     \"n_estimators\": np.arange(10, 200, 20),  # Number of estimators in BaggingClassifier\n",
    "# }\n",
    "\n",
    "# # Setup BaggingClassifier with SVC as the base estimator\n",
    "# bagging_svc = BaggingClassifier(base_estimator=SVC(), random_state=42)\n",
    "\n",
    "# # Setup random hyperparameter search for BaggingClassifier with SVC\n",
    "# rs_bagging_svc = RandomizedSearchCV(bagging_svc,\n",
    "#                                     param_distributions=svc_grid,\n",
    "#                                     cv=5,\n",
    "#                                     n_iter=20,\n",
    "#                                     verbose=True,\n",
    "#                                     random_state=42)\n",
    "\n",
    "# # Fit the RandomizedSearchCV model\n",
    "# rs_bagging_svc.fit(X_train, y_train)\n",
    "\n",
    "# # Best hyperparameters and model score\n",
    "# print(\"Best parameters for BaggingClassifier with SVC:\", rs_bagging_svc.best_params_)\n",
    "# print(\"BaggingClassifier (SVC) model test score:\", rs_bagging_svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Tuning models with GridSearchCV\n",
    "\n",
    "The difference between RandomizedSearchCV and GridSearchCV is:\n",
    "\n",
    "- sklearn.model_selection.RandomizedSearchCV searches over a grid of hyperparameters performing n_iter combinations (e.g. will explore random combinations of the hyperparameters for a defined number of iterations).\n",
    "- sklearn.model_selection.GridSearchCV will test every single possible combination of hyperparameters in the grid (this is a thorough test but can take quite a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42) \n",
    "\n",
    "# Different LogisticRegression hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the best parameters\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "gs_log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters for KNN\n",
    "knn_grid = {\n",
    "    \"n_neighbors\": np.arange(1, 21),  # Number of neighbors to use\n",
    "    \"weights\": [\"uniform\", \"distance\"],  # Weighting function for neighbors\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],  # Algorithm for computing nearest neighbors\n",
    "    \"leaf_size\": np.arange(20, 60, 5),  # Leaf size for tree algorithms\n",
    "    \"p\": [1, 2]  # Power parameter for Minkowski distance (1=Manhattan, 2=Euclidean)\n",
    "}\n",
    "\n",
    "# Setup grid hyperparameter search for KNN\n",
    "gs_knn = GridSearchCV(KNeighborsClassifier(),\n",
    "                      param_grid=knn_grid,\n",
    "                      cv=5,  # 5-fold cross-validation\n",
    "                      verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for KNN\n",
    "gs_knn.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model evaluation\n",
    "print(\"Best parameters for KNN:\", gs_knn.best_params_)\n",
    "knn_best_model = gs_knn.best_estimator_\n",
    "print(\"KNN test score:\", knn_best_model.score(X_test, y_test))\n",
    "\n",
    "# Evaluate KNN model\n",
    "knn_predictions = knn_best_model.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "knn_recall = recall_score(y_test, knn_predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters for RandomForestClassifier\n",
    "rf_grid = {\n",
    "    \"n_estimators\": [100, 200],  # Number of trees in the forest\n",
    "    \"max_depth\": [None, 10, 20],  # Maximum depth of the tree\n",
    "    \"min_samples_split\": [2, 10],  # Minimum number of samples required to split a node\n",
    "    \"min_samples_leaf\": [1, 4],  # Minimum number of samples required at each leaf node\n",
    "    \"bootstrap\": [True]  # Whether bootstrap samples are used\n",
    "}\n",
    "\n",
    "# Setup grid hyperparameter search for RandomForestClassifier\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                     param_grid=rf_grid,\n",
    "                     cv=5,  # 5-fold cross-validation\n",
    "                     verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for RandomForestClassifier\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model evaluation\n",
    "print(\"Best parameters for RandomForestClassifier:\", gs_rf.best_params_)\n",
    "rf_best_model = gs_rf.best_estimator_\n",
    "print(\"RandomForestClassifier test score:\", gs_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define hyperparameters for ANN (MLPClassifier)\n",
    "ann_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,)],  # Different layer sizes\n",
    "    \"activation\": [\"relu\"],  # Activation functions\n",
    "    \"solver\": [\"adam\"],  # Optimization solvers\n",
    "    \"alpha\":  [0.001, 0.01, 0.1],  # Focused range\n",
    "    \"learning_rate\": [\"adaptive\"],  # Learning rate strategy\n",
    "    \"max_iter\": [500]  # Max iterations for convergence\n",
    "}\n",
    "\n",
    "# Setup grid hyperparameter search for ANN (MLPClassifier)\n",
    "gs_ann = GridSearchCV(MLPClassifier(),\n",
    "                      param_grid=ann_grid,\n",
    "                      cv=5,\n",
    "                      verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for ANN\n",
    "gs_ann.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for ANN:\", gs_ann.best_params_)\n",
    "print(\"ANN test score:\", gs_ann.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters for Naive Bayes (GaussianNB)\n",
    "nb_gaussian_grid = {\n",
    "    \"var_smoothing\": np.logspace(-9, 0, 10)  # Smoothing parameter\n",
    "}\n",
    "\n",
    "# Setup grid hyperparameter search for Naive Bayes (GaussianNB)\n",
    "gs_nb_gaussian = GridSearchCV(GaussianNB(),\n",
    "                              param_grid=nb_gaussian_grid,\n",
    "                              cv=5,\n",
    "                              verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for Naive Bayes (GaussianNB)\n",
    "gs_nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Naive Bayes (GaussianNB):\", gs_nb_gaussian.best_params_)\n",
    "print(\"Naive Bayes (GaussianNB) test score:\", gs_nb_gaussian.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters for Naive Bayes (BernoulliNB)\n",
    "nb_bernoulli_grid = {\n",
    "    \"alpha\": np.logspace(-4, 4, 20),  # Additive smoothing parameter\n",
    "    \"binarize\": [0.0, 0.1, 0.2, 0.3],  # Threshold for binarizing input\n",
    "}\n",
    "\n",
    "# Setup grid hyperparameter search for Naive Bayes (BernoulliNB)\n",
    "gs_nb_bernoulli = GridSearchCV(BernoulliNB(),\n",
    "                               param_grid=nb_bernoulli_grid,\n",
    "                               cv=5,\n",
    "                               verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for Naive Bayes (BernoulliNB)\n",
    "gs_nb_bernoulli.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Naive Bayes (BernoulliNB):\", gs_nb_bernoulli.best_params_)\n",
    "print(\"Naive Bayes (BernoulliNB) test score:\", gs_nb_bernoulli.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate classification error\n",
    "def classification_error(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Define a function to calculate specificity\n",
    "def specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum(axis=1) - cm.diagonal()  # True negatives for each class\n",
    "    fp = cm.sum(axis=0) - cm.diagonal()  # False positives for each class\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)  # Avoid division by zero\n",
    "    return specificity_per_class.mean()  # Average specificity across all classes\n",
    "\n",
    "tuned_model_scores = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_log_reg.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_log_reg.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_log_reg.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_knn.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_knn.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_knn.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_knn.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_rf.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_rf.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_rf.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_dt.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_dt.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_dt.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (GaussianNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_nb_gaussian.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_nb_gaussian.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_nb_gaussian.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (BernoulliNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_nb_bernoulli.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_nb_bernoulli.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_nb_bernoulli.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_ann.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_ann.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_ann.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_ann.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_ann.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_ann.predict(X_test), average=\"weighted\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert results to a DataFrame and scale to percentages\n",
    "tuned_model_comparison = pd.DataFrame(tuned_model_scores).T * 100\n",
    "\n",
    "# Display the DataFrame\n",
    "print(tuned_model_comparison)\n",
    "\n",
    "# Optional: Pretty-print the table\n",
    "tuned_model_comparison.style.format({\n",
    "    \"Accuracy\": \"{:.2f}%\",\n",
    "    \"Precision\": \"{:.2f}%\",\n",
    "    \"Sensitivity (Recall)\": \"{:.2f}%\",\n",
    "    \"Classification Error\": \"{:.2f}%\",\n",
    "    \"Specificity\": \"{:.2f}%\",\n",
    "    \"F1 Measure\": \"{:.2f}%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluating a classification model, beyond accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preidctions on test data\n",
    "y_preds = gs_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ROC Curve and AUC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay \n",
    "\n",
    "# from_estimator() = use a model to plot ROC curve on data\n",
    "RocCurveDisplay.from_estimator(estimator=gs_log_reg, \n",
    "                               X=X_test, \n",
    "                               y=y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Creating a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix\n",
    "print(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5) # Increase font size\n",
    "\n",
    "def plot_conf_mat(y_test, y_preds):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using Seaborn's heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n",
    "                     annot=True, # Annotate the boxes\n",
    "                     cbar=False)\n",
    "    plt.xlabel(\"true label\")\n",
    "    plt.ylabel(\"predicted label\")\n",
    "    \n",
    "plot_conf_mat(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification report\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check best hyperparameters\n",
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Instantiate best model with best hyperparameters (found with GridSearchCV)\n",
    "clf = LogisticRegression(C=0.23357214690901212,\n",
    "                         solver=\"liblinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Cross-validated accuracy score\n",
    "cv_acc = cross_val_score(clf,\n",
    "                         X,\n",
    "                         y,\n",
    "                         cv=5, # 5-fold cross-validation, this is the default\n",
    "                         scoring=\"accuracy\") # accuracy as scoring\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc = np.mean(cv_acc)\n",
    "cv_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated precision score\n",
    "cv_precision = np.mean(cross_val_score(clf,\n",
    "                                       X,\n",
    "                                       y,\n",
    "                                       cv=5, # 5-fold cross-validation\n",
    "                                       scoring=\"precision\")) # precision as scoring\n",
    "cv_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated recall score\n",
    "cv_recall = np.mean(cross_val_score(clf,\n",
    "                                    X,\n",
    "                                    y,\n",
    "                                    cv=5, # 5-fold cross-validation\n",
    "                                    scoring=\"recall\")) # recall as scoring\n",
    "cv_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated F1 score\n",
    "cv_f1 = np.mean(cross_val_score(clf,\n",
    "                                X,\n",
    "                                y,\n",
    "                                cv=5, # 5-fold cross-validation\n",
    "                                scoring=\"f1\")) # f1 as scoring\n",
    "cv_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing cross-validated metrics\n",
    "cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n",
    "                            \"Precision\": cv_precision,\n",
    "                            \"Recall\": cv_recall,\n",
    "                            \"F1\": cv_f1},\n",
    "                          index=[0])\n",
    "cv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an instance of LogisticRegression (taken from above)\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coef_\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match features to columns\n",
    "features_dict = dict(zip(df.columns, list(clf.coef_[0])))\n",
    "features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "features_df = pd.DataFrame(features_dict, index=[0])\n",
    "features_df.T.plot.bar(title=\"Feature Importance\", legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"sex\"], df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrast slope (positive coefficient) with target\n",
    "pd.crosstab(df[\"slope\"], df[\"target\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
