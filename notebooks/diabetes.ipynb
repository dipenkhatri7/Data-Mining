{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparing the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Regular EDA and plotting libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "## Models\n",
    "import sklearn \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier # For ANN\n",
    "from sklearn.svm import SVC  # For SVM (Support Vector Machine)\n",
    "from sklearn.naive_bayes import GaussianNB  # For Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB  # For binary classification\n",
    "\n",
    "\n",
    "## Model evaluators\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "import datetime\n",
    "print(f\"Notebook last updated: {datetime.datetime.now()}\\n\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
    "print(f\"Scikit-Learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get project root directory\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from src import config\n",
    "\n",
    "# Use absolute path instead of relative\n",
    "df = pd.read_csv(os.path.join(project_dir, config.DIABETES_DATA_PATH))\n",
    "\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Exploration (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of positive(1) and negative(0) samples in our dataframe\n",
    "df.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized value counts\n",
    "df.Outcome.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Outcome.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"])\n",
    "plt.title(\"Diabetes Frequency\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Comparing one feature to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age bins and labels\n",
    "age_bins = [0, 20, 40, 60, 80, 100]\n",
    "age_labels = ['0-20', '20-40', '40-60', '60-80', '80-100']\n",
    "\n",
    "pd.crosstab(\n",
    "    df['Outcome'], \n",
    "    pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for \"Disease\"\n",
    "plt.scatter(\n",
    "    df.Age[df.Outcome == 1],\n",
    "    df.BloodPressure[df.Outcome == 1],\n",
    "    c=\"#ff7f0e\",  # CUD Orange\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# Scatter plot for \"No Disease\"\n",
    "plt.scatter(\n",
    "    df.Age[df.Outcome == 0],\n",
    "    df.BloodPressure[df.Outcome == 0],\n",
    "    c=\"#1f77b4\",  # CUD Blue\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.title(\"Diabetes by Age and Blood Pressure\", fontsize=12)\n",
    "plt.xlabel(\"Age\", fontsize=10)\n",
    "plt.ylabel(\"Blood Pressure\", fontsize=10)\n",
    "\n",
    "plt.legend([\"Disease\", \"No Disease\"], fontsize=9)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.grid(visible=True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Diabetes frequency and DiabetesPedigreeFunction\n",
    "\n",
    "print(pd.crosstab(index=df.Pregnancies, columns=df.Outcome))\n",
    "\n",
    "# Create a new crosstab and base plot\n",
    "pd.crosstab(df.Pregnancies, df.Outcome).plot(kind=\"bar\", figsize=(10,6), color=[\"lightblue\", \"salmon\"])\n",
    "\n",
    "# Add attributes to the plot to make it more readable\n",
    "plt.title(\"Diabetes Frequency Per Pregnancy\")\n",
    "plt.xlabel(\"Number of Pregnancies\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend([\"Non-Diabetic\", \"Diabetic\"])\n",
    "plt.xticks(rotation = 0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing Age and Glucose\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Scatter Plot for \"Disease\"\n",
    "plt.scatter(df.Age[df.Outcome==1], \n",
    "            df.Glucose[df.Outcome==1], \n",
    "            c=\"#ff7f0e\",  # CUD Orange\n",
    "            alpha=0.8,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5) # define it as a scatter figure\n",
    "\n",
    "# Scatter Plot for \"No Disease\"\n",
    "plt.scatter(df.Age[df.Outcome==0], \n",
    "            df.Glucose[df.Outcome==0], \n",
    "            c=\"#1f77b4\",  # CUD Blue\n",
    "            alpha=0.8,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5) # axis always come as (x, y)\n",
    "\n",
    "# Add some helpful info\n",
    "plt.title(\"Diabetes outcome in function of Age and Glucose\")\n",
    "plt.xlabel(\"Age\", fontsize=10)\n",
    "plt.ylabel(\"Glucose\", fontsize=10)\n",
    "\n",
    "plt.legend([\"Disease\", \"No Disease\"], fontsize=12)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "plt.grid(visible=True, linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Histogram to check the distribution of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_histograms_for_paper(df, columns, output_dir, bins=15, color='#1f77b4', figsize=(3, 2)):\n",
    "    \"\"\"    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing the data.\n",
    "    - columns (list): List of column names to plot.\n",
    "    - output_dir (str): Directory to save the images.\n",
    "    - bins (int): Number of bins for the histograms.\n",
    "    - color (str): Color of the histograms (color-blind friendly).\n",
    "    - figsize (tuple): Size of each figure (width, height).\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.histplot(\n",
    "            df[col],\n",
    "            bins=bins,\n",
    "            kde=False,\n",
    "            color=color,\n",
    "            edgecolor='black'\n",
    "        )\n",
    "        \n",
    "        # Remove axis labels\n",
    "        plt.xlabel(\"\")  # Remove x-axis label text\n",
    "        plt.ylabel(\"\")  # Remove y-axis label text\n",
    "        \n",
    "        # Add title\n",
    "        plt.title(col, fontsize=12)\n",
    "        \n",
    "        # Enable grid\n",
    "        plt.grid(visible=True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "        \n",
    "        # Format layout\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(f\"{output_dir}/{col}_histogram.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "# List of columns to plot\n",
    "columns_to_plot = [\n",
    "    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', \n",
    "    'BMI', 'DiabetesPedigreeFunction', 'Age'\n",
    "]\n",
    "\n",
    "output_directory = os.path.join(project_dir, \"reports\", \"figures\", \"histograms\", \"diabetes\")\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Call function to save histograms\n",
    "save_histograms_for_paper(df, columns_to_plot, output_directory, bins=15,  color='#1f77b4', figsize=(3.5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Correlation between independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between our independent variables\n",
    "X = df.drop(columns=['Outcome'])\n",
    "y = df['Outcome'].values\n",
    "\n",
    "corr_matrix = X.corr()\n",
    "corr_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True,\n",
    "    linewidths=0.5,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",  # Use a colorblind-friendly palett\n",
    "    annot_kws={\"size\": 11},\n",
    ")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features and target variables.\n",
    "\n",
    "X = df.drop(labels=\"Outcome\", axis=1)\n",
    "\n",
    "# Target variable (in the form of a NumPy array)\n",
    "y = df.Outcome.to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creating a training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test set\n",
    "# Random seed for reproducible results\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n",
    "                                                    y, # dependent variable\n",
    "                                                    test_size = 0.2, # percentage of data to use for test set\n",
    "                                                    stratify=y) # keep the same proportion of target as the original dataset instead of random\n",
    "\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data of independent variable\n",
    "print(X_train.shape)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data of dependent variable\n",
    "y_train, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data of independent variable\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data of dependent variable\n",
    "y_test, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Choosing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by trying the following models and comparing their results.\n",
    "\n",
    "1. K-Nearest Neighbors - sklearn.neighbors.KNeighboursClassifier()\n",
    "2. Logistic Regression - sklearn.linear_model.LogisticRegression()\n",
    "3. RandomForest - sklearn.ensemble.RandomForestClassifier()\n",
    "4. Decision Tree: sklearn.tree.DecisionTreeClassifier()\n",
    "5. SVC: sklearn.svm.SVC()\n",
    "6. ANN: MLPClassifier()\n",
    "7. Naive Bayes (Gaussian): sklearn.naive_bayes.GaussianNB()\n",
    "8. Naive Bayes (Bernoulli): sklearn.naive_bayes.BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put models in a dictionary\n",
    "models = {\"KNN\": KNeighborsClassifier(),\n",
    "          \"Logistic Regression\": LogisticRegression(max_iter=2000, C=1.0), # Note: if you see a warning about \"convergence not reached\", you can increase `max_iter` until convergence is reached\n",
    "          \"Random Forest\": RandomForestClassifier(),\n",
    "          \"Decision Tree\": DecisionTreeClassifier(),\n",
    "          \"SVC\": SVC(),\n",
    "          \"ANN\": MLPClassifier(max_iter=1000),\n",
    "          \"Naive Bayes (Gaussian)\": GaussianNB(),\n",
    "          \"Naive Bayes (Bernoulli)\": BernoulliNB()}\n",
    "\n",
    "# Create function to fit and score models\n",
    "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Fits and evaluates given machine learning models.\n",
    "    # models : a dict of different Scikit-Learn machine learning models\n",
    "    # X_train : training data\n",
    "    # X_test : testing data\n",
    "    # y_train : labels assosciated with training data\n",
    "    # y_test : labels assosciated with test data\n",
    "    \n",
    "    # Random seed for reproducible results\n",
    "    np.random.seed(42)\n",
    "    # Make a list to keep model scores\n",
    "    model_scores = {}\n",
    "    # Loop through models\n",
    "    for name, model in models.items():\n",
    "        # Fit the model to the data\n",
    "        model.fit(X_train, y_train)\n",
    "        # Evaluate the model and append its score to model_scores\n",
    "        model_scores[name] = model.score(X_test, y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = fit_and_score(models=models, \n",
    "                             X_train=X_train, \n",
    "                             X_test=X_test, \n",
    "                             y_train=y_train, \n",
    "                             y_test=y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores = {\n",
    "    name: recall_score(y_test, model.predict(X_test), average='weighted')\n",
    "    for name, model in models.items()\n",
    "}\n",
    "\n",
    "recall_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_scores = {\n",
    "    name: precision_score(y_test, model.predict(X_test), average='weighted')\n",
    "    for name, model in models.items()\n",
    "}\n",
    "precision_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Comparing the results of several models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n",
    "model_compare.T.plot.bar()\n",
    "\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "\n",
    "# Set y-axis limits and format to percentage\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y*100:.0f}%'))\n",
    "\n",
    "plt.gca().get_legend().remove()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Some more exaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {\n",
    "    name: f1_score(y_test, model.predict(X_test), average='weighted')\n",
    "    for name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity_scores = {}\n",
    "for name, model in models.items():\n",
    "    cm = confusion_matrix(y_test, model.predict(X_test))\n",
    "    tn = cm[0][0]\n",
    "    fp = cm[0][1]\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    specificity_scores[name] = specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_error_scores = {\n",
    "    name: 1 - accuracy\n",
    "    for name, accuracy in model_scores.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison = pd.DataFrame({\n",
    "    \"Accuracy (%)\": [score * 100 for score in model_scores.values()],\n",
    "    \"Recall (%)\": [score * 100 for score in recall_scores.values()],\n",
    "     \"Precision (%)\": [score * 100 for score in precision_scores.values()],\n",
    "    \"F1 Score (%)\": [score * 100 for score in f1_scores.values()],\n",
    "    \"Specificity (%)\": [score * 100 for score in specificity_scores.values()],\n",
    "    \"Classification Error (%)\": [error * 100 for error in classification_error_scores.values()]\n",
    "    \n",
    "}, index=model_scores.keys())\n",
    "\n",
    "# Optional: Pretty-print the table\n",
    "model_comparison.style.format({\"Accuracy (%)\": \"{:.2f}\", \n",
    "                               \"Recall (%)\": \"{:.2f}\",\n",
    "                               \"Precision (%)\": \"{:.2f}\", \n",
    "                               \"F1 Score (%)\": \"{:.2f}\", \n",
    "                               \"Specificity (%)\": \"{:.2f}\", \n",
    "                               \"Classification Error (%)\": \"{:.2f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Ensemble Method: Max Voting (Hard & Soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define other classifiers for VotingClassifier\n",
    "log_reg = LogisticRegression(max_iter=1500)\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "svc = SVC(probability=True)\n",
    "naive_bayes = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Setup VotingClassifier with hard voting\n",
    "voting_hard = VotingClassifier(estimators=[('log_reg', log_reg), ('rf', rf), ('svc', svc), ('naive_bayes', naive_bayes), ('knn', knn)], voting='hard')\n",
    "\n",
    "voting_hard.fit(X_train, y_train)\n",
    "\n",
    "print(f\"VotingClassifier with hard voting model test score: {voting_hard.score(X_test, y_test)}\")\n",
    "print(f\"VotingClassifier with hard voting model test recall score: {recall_score(y_test, voting_hard.predict(X_test), average='weighted')}\")\n",
    "print(f\"VotingClassifier with hard voting model test precision score: {precision_score(y_test, voting_hard.predict(X_test), average='weighted')}\")\n",
    "\n",
    "# Setup VotingClassifier with soft voting\n",
    "voting_soft = VotingClassifier(estimators=[('log_reg', log_reg), ('rf', rf), ('svc', svc), ('naive_bayes', naive_bayes), ('knn', knn)], voting='soft')\n",
    "\n",
    "voting_soft.fit(X_train, y_train)\n",
    "\n",
    "print(f\"VotingClassifier with soft voting model test score: {voting_soft.score(X_test, y_test)}\")\n",
    "print(f\"VotingClassifier with soft voting model test recall score: {recall_score(y_test, voting_soft.predict(X_test), average='weighted')}\")\n",
    "print(f\"VotingClassifier with soft voting model test precision score: {precision_score(y_test, voting_soft.predict(X_test), average='weighted')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KNN hyperparameters\n",
    "knn_grid = {\n",
    "    \"n_neighbors\": np.arange(1, 21),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n",
    "    \"leaf_size\": np.arange(20, 60, 5),\n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "\n",
    "# Define LogisticRegression hyperparameters\n",
    "log_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n",
    "                \"solver\": [\"liblinear\"]}\n",
    "\n",
    "# Define RandomForestClassifier hyperparameters\n",
    "rf_grid = {\n",
    "    \"n_estimators\": np.arange(50, 500, 50),  \n",
    "    \"max_depth\": [None, 5, 10],              \n",
    "    \"min_samples_split\": np.arange(2, 10, 2),\n",
    "    \"min_samples_leaf\": np.arange(1, 10, 2)\n",
    "}\n",
    "\n",
    "# Define DecisionTreeClassifier hyperparameter\n",
    "dt_grid = {\n",
    "    \"max_depth\": [None, 3, 5, 10, 20],\n",
    "    \"min_samples_split\": np.arange(2, 20, 2),\n",
    "    \"min_samples_leaf\": np.arange(1, 20, 2),\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# Define SVC linear hyperparameters\n",
    "svc_grid_linear = {\n",
    "    \"C\": np.logspace(-2, 2, 5),\n",
    "    \"kernel\": [\"linear\"],\n",
    "    \"gamma\": [\"scale\"]\n",
    "}\n",
    "\n",
    "# Define SVC RBF hyperparameters\n",
    "svc_grid_rbf = {\n",
    "    \"C\": np.logspace(-4, 4, 20),\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# Define SVC poly hyperparameters\n",
    "svc_grid_poly = {\n",
    "    \"C\": np.logspace(-2, 2, 5),\n",
    "    \"kernel\": [\"poly\"],\n",
    "    \"degree\": [2, 3],  # Fewer degrees\n",
    "    \"gamma\": [\"scale\"]\n",
    "}\n",
    "\n",
    "# Define SVC sigmoid hyperparameters\n",
    "svc_grid_sigmoid = {\n",
    "    \"C\": np.logspace(-2, 2, 5),\n",
    "    \"kernel\": [\"sigmoid\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# Define ANN hyperparameters\n",
    "ann_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (50, 50)],\n",
    "    \"activation\": [\"tanh\", \"relu\"], # Activation function\n",
    "    \"solver\": [\"adam\", \"sgd\"],\n",
    "    \"alpha\": np.logspace(-4, 4, 10),\n",
    "    \"learning_rate\": [\"constant\", \"adaptive\"]\n",
    "}\n",
    "\n",
    "# Define Naive Bayes (Gaussian) hyperparameters\n",
    "nb_gaussian_grid = {\n",
    "    \"var_smoothing\": np.logspace(-9, 0, 10)  # Smoothing parameter\n",
    "}\n",
    "\n",
    "# Define Naive Bayes (Bernoulli) hyperparameters\n",
    "nb_bernoulli_grid = {\n",
    "    \"alpha\": np.logspace(-4, 4, 20),  # Additive smoothing parameter\n",
    "    \"binarize\": [0.0, 0.1, 0.2, 0.3],  # Threshold for binarizing input\n",
    "}\n",
    "\n",
    "# Define VortingClassifier hyperparameters\n",
    "voting_grid = {\n",
    "    'voting': ['hard', 'soft'],  # Hard and soft voting\n",
    "    'weights': [[1, 1, 1, 1], [2, 1, 1, 1], [1, 2, 1, 1], [1, 1, 2, 1]],  # Weighted voting\n",
    "    'flatten_transform': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Tune KNeighborsClassifier (KNN) by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of train scores\n",
    "train_scores = []\n",
    "\n",
    "# Create a list of test scores\n",
    "test_scores = []\n",
    "\n",
    "# Create a list of different values for n_neighbors\n",
    "neighbors = range(1, 21) # 1 to 20\n",
    "\n",
    "# Setup algorithm\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Loop through different neighbors values\n",
    "for i in neighbors:\n",
    "    knn.set_params(n_neighbors = i) # set neighbors value\n",
    "    \n",
    "    # Fit the algorithm\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Update the training scores\n",
    "    train_scores.append(knn.score(X_train, y_train))\n",
    "    \n",
    "    # Update the test scores\n",
    "    test_scores.append(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN's train scores\n",
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize KNN score test and train data\n",
    "plt.plot(neighbors, train_scores, label=\"Train score\")\n",
    "plt.plot(neighbors, test_scores, label=\"Test score\")\n",
    "plt.xticks(np.arange(1, 21, 1))\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Model score\")\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tuning models with with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsClassifier with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for KNN\n",
    "rs_knn = RandomizedSearchCV(KNeighborsClassifier(),\n",
    "                            param_distributions=knn_grid,\n",
    "                            cv=5,\n",
    "                            n_iter=20,\n",
    "                            verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for KNN with RandomizedSearchCV:\", rs_knn.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (KNN) model test score:\", rs_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for LogisticRegression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),\n",
    "                                param_distributions=log_reg_grid,\n",
    "                                cv=5,\n",
    "                                n_iter=20,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_log_reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Logistic Regression with RandomizedSearchCV:\", rs_log_reg.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (Logistic Regression) model test score:\", rs_log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for RandomForestClassifier\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n",
    "                           param_distributions=rf_grid,\n",
    "                           cv=5,\n",
    "                           n_iter=20,\n",
    "                           verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters for Random Forest with RandomizedSearchCV:', rs_rf.best_params_)\n",
    "\n",
    "print('RandomizedSearchCV (Random Forest) model test score:', rs_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for DecisionTreeClassifier\n",
    "rs_dt = RandomizedSearchCV(\n",
    "    DecisionTreeClassifier(),\n",
    "    param_distributions=dt_grid,\n",
    "    cv=5,\n",
    "    n_iter=20,\n",
    "    verbose=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the random hyperparameter search model\n",
    "rs_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Decision Tree with RandomizedSearchCV:\", rs_dt.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (Decision Tree) model test score:\", rs_dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with linear kernel and RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for SVC\n",
    "rs_svc_linear = RandomizedSearchCV(SVC(),\n",
    "                            param_distributions=svc_grid_linear,\n",
    "                            cv=3,\n",
    "                            n_iter=10,\n",
    "                            verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_svc_linear.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (linear) with RandomizedSearchCV:\", rs_svc_linear.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (SVC linear) model test score:\", rs_svc_linear.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with RBF kernel and RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for SVC\n",
    "rs_svc_rbf = RandomizedSearchCV(SVC(),\n",
    "                                param_distributions=svc_grid_rbf,\n",
    "                                cv=3,\n",
    "                                n_iter=10,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_svc_rbf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (RBF) with RandomizedSearchCV:\", rs_svc_rbf.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (SVC RBF) model test score:\", rs_svc_rbf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVC with poly kernel and RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for SVC\n",
    "rs_svc_poly = RandomizedSearchCV(SVC(),\n",
    "                                param_distributions=svc_grid_poly,\n",
    "                                cv=3,\n",
    "                                n_iter=5,\n",
    "                                verbose=True,\n",
    "                                n_jobs=-1) # Use all available CPU cores\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_svc_poly.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (poly) with RandomizedSearchCV:\", rs_svc_poly.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (SVC poly) model test score:\", rs_svc_poly.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with sigmoid kernel and RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for SVC\n",
    "rs_svc_sigmoid = RandomizedSearchCV(SVC(),\n",
    "                                param_distributions=svc_grid_sigmoid,\n",
    "                                cv=3,\n",
    "                                n_iter=10,\n",
    "                                verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_svc_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (sigmoid) with RandomizedSearchCV:\", rs_svc_sigmoid.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (SVC sigmoid) model test score:\", rs_svc_sigmoid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for ANN\n",
    "rs_ann = RandomizedSearchCV(MLPClassifier(max_iter=3000),\n",
    "                            param_distributions=ann_grid,\n",
    "                            cv=5,\n",
    "                            n_iter=20,\n",
    "                            verbose=True)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_ann.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for ANN with RandomizedSearchCV:\", rs_ann.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (ANN) model test score:\", rs_ann.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for GaussianNB\n",
    "rs_nb_gaussian = RandomizedSearchCV(GaussianNB(), \n",
    "                                    param_distributions=nb_gaussian_grid, \n",
    "                                    cv=5, \n",
    "                                    n_iter=10, \n",
    "                                    verbose=True, \n",
    "                                    random_state=42)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Gaussian Naive Bayes with RandomizedSearchCV:\", rs_nb_gaussian.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (Gaussian Naive Bayes) model test score:\", rs_nb_gaussian.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for BernoulliNB\n",
    "rs_nb_bernoulli = RandomizedSearchCV(BernoulliNB(), \n",
    "                                     param_distributions=nb_bernoulli_grid, \n",
    "                                     cv=5, \n",
    "                                     n_iter=20, \n",
    "                                     verbose=True, \n",
    "                                     random_state=42)\n",
    "\n",
    "# Fit random hyperparameter search model\n",
    "rs_nb_bernoulli.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Bernoulli Naive Bayes with RandomizedSearchCV:\", rs_nb_bernoulli.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (Bernoulli Naive Bayes) model test score:\", rs_nb_bernoulli.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VotingClassifier with RandomizedSearchCV\n",
    "\n",
    "# Setup random hyperparameter search for VotingClassifier\n",
    "rs_voting = RandomizedSearchCV(VotingClassifier(estimators=[('log_reg', log_reg), ('rf', rf), \n",
    "                                                            # ('svc', svc), \n",
    "                                                            ('naive_bayes', naive_bayes), ('knn', knn)], voting='soft'),\n",
    "                               param_distributions=voting_grid,\n",
    "                               cv=5,\n",
    "                               n_iter=10,\n",
    "                               verbose=True)\n",
    "\n",
    "rs_voting.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for VotingClassifier with RandomizedSearchCV:\", rs_voting.best_params_)\n",
    "\n",
    "print(\"RandomizedSearchCV (VotingClassifier) model test score:\", rs_voting.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate classification error and specificity\n",
    "\n",
    "def classification_error(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum(axis=1) - cm.diagonal()  # True negatives for each class\n",
    "    fp = cm.sum(axis=0) - cm.diagonal()  # False positives for each class\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)  # Avoid division by zero\n",
    "    return specificity_per_class.mean()  # Average specificity across all classes\n",
    "\n",
    "tuned_model_scores = {\n",
    "     \"KNN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_knn.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_knn.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_knn.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_knn.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_log_reg.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_log_reg.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_log_reg.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_rf.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_rf.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_rf.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_dt.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_dt.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_dt.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (Linear)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_svc_linear.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_svc_linear.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_svc_linear.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_svc_linear.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_svc_linear.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_svc_linear.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (RBF)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_svc_rbf.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_svc_rbf.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_svc_rbf.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_svc_rbf.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_svc_rbf.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_svc_rbf.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (Poly)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_svc_poly.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_svc_poly.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_svc_poly.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_svc_poly.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_svc_poly.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_svc_poly.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (Sigmoid)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_svc_sigmoid.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_svc_sigmoid.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_svc_sigmoid.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_svc_sigmoid.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_svc_sigmoid.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_svc_sigmoid.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_ann.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_ann.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_ann.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_ann.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_ann.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_ann.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (GaussianNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_nb_gaussian.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_nb_gaussian.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_nb_gaussian.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (BernoulliNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_nb_bernoulli.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_nb_bernoulli.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_nb_bernoulli.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"VotingClassifier\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_voting.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_voting.predict(X_test), average=\"weighted\"),\n",
    "        \"Recall (Sensitivity)\": recall_score(y_test, rs_voting.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_voting.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_voting.predict(X_test)),\n",
    "        \"F1 Score\": f1_score(y_test, rs_voting.predict(X_test), average=\"weighted\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "tuned_model_comparison = pd.DataFrame(tuned_model_scores).T * 100  # Convert to percentages\n",
    "\n",
    "# Optional: Pretty-print the table\n",
    "tuned_model_comparison.style.format({\"Accuracy\": \"{:.2f}%\",\n",
    "    \"Precision\": \"{:.2f}%\",\n",
    "    \"Recall (Sensitivity)\": \"{:.2f}%\",\n",
    "    \"Classification Error\": \"{:.2f}%\",\n",
    "    \"Specificity\": \"{:.2f}%\",\n",
    "    \"F1 Score\": \"{:.2f}%\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Tuning models with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between RandomizedSearchCV and GridSearchCV is:\n",
    "\n",
    "- sklearn.model_selection.RandomizedSearchCV searches over a grid of hyperparameters performing n_iter combinations (e.g. will explore random combinations of the hyperparameters for a defined number of iterations).\n",
    "- sklearn.model_selection.GridSearchCV will test every single possible combination of hyperparameters in the grid (this is a thorough test but can take quite a long time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNeighborsClassifier with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for KNN\n",
    "gs_knn = GridSearchCV(KNeighborsClassifier(),\n",
    "                      param_grid=knn_grid,\n",
    "                      cv=5,\n",
    "                      verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for KNN\n",
    "gs_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for KNN with GridSearchCV:\", gs_knn.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (KNN) model test score:\", gs_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for LogisticRegression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),\n",
    "                          param_grid=log_reg_grid,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search model\n",
    "gs_log_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters for Logistic Regression with GridSearchCV:\", gs_log_reg.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (Logistic Regression) model test score:\", gs_log_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for RandomForestClassifier\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(),\n",
    "                     param_grid=rf_grid,\n",
    "                     cv=3,\n",
    "                     n_jobs=-1,\n",
    "                     verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for RandomForestClassifier\n",
    "gs_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Random Forest with GridSearchCV:\", gs_rf.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (Random Forest) model test score:\", gs_rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for DecisionTreeClassifier\n",
    "gs_dt = GridSearchCV(DecisionTreeClassifier(),\n",
    "                     param_grid=dt_grid,\n",
    "                     cv=5,\n",
    "                     verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for DecisionTreeClassifier\n",
    "gs_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Decision Tree with GridSearchCV:\", gs_dt.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (Decision Tree) model test score:\", gs_dt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with linear kernel and GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for SVC\n",
    "gs_svc_linear = GridSearchCV(SVC(),\n",
    "                             param_grid=svc_grid_linear,\n",
    "                             cv=5,\n",
    "                             verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for SVC\n",
    "gs_svc_linear.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (linear) with GridSearchCV:\", gs_svc_linear.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (SVC linear) model test score:\", gs_svc_linear.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with RBF kernel and GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for SVC\n",
    "gs_svc_rbf = GridSearchCV(SVC(),\n",
    "                          param_grid=svc_grid_rbf,\n",
    "                          cv=5,\n",
    "                          verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for SVC\n",
    "gs_svc_rbf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (RBF) with GridSearchCV:\", gs_svc_rbf.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (SVC RBF) model test score:\", gs_svc_rbf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with poly kernel and GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for SVC\n",
    "gs_svc_poly = GridSearchCV(SVC(),\n",
    "                           param_grid=svc_grid_poly,\n",
    "                           cv=5,\n",
    "                           verbose=True,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "# Fit grid hyperparameter search for SVC\n",
    "gs_svc_poly.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (poly) with GridSearchCV:\", gs_svc_poly.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (SVC poly) model test score:\", gs_svc_poly.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC with sigmoid kernel and GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for SVC\n",
    "gs_svc_sigmoid = GridSearchCV(SVC(),\n",
    "                              param_grid=svc_grid_sigmoid,\n",
    "                              cv=5,\n",
    "                              verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for SVC\n",
    "gs_svc_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for SVC (sigmoid) with GridSearchCV:\", gs_svc_sigmoid.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (SVC sigmoid) model test score:\", gs_svc_sigmoid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for ANN (MLPClassifier)\n",
    "gs_ann = GridSearchCV(MLPClassifier(max_iter=5000),\n",
    "                      param_grid=ann_grid,\n",
    "                      cv=5,\n",
    "                      verbose=True,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "# Fit grid hyperparameter search for ANN\n",
    "gs_ann.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for ANN with GridSearchCV:\", gs_ann.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (ANN) model test score:\", gs_ann.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes (GaussianNB) with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for Naive Bayes (GaussianNB)\n",
    "gs_nb_gaussian = GridSearchCV(GaussianNB(),\n",
    "                              param_grid=nb_gaussian_grid,\n",
    "                              cv=5,\n",
    "                              verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for Naive Bayes (GaussianNB)\n",
    "gs_nb_gaussian.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Gaussian Naive Bayes with GridSearchCV:\", gs_nb_gaussian.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (Gaussian Naive Bayes) model test score:\", gs_nb_gaussian.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes (BernoulliNB) with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for Naive Bayes (BernoulliNB)\n",
    "gs_nb_bernoulli = GridSearchCV(BernoulliNB(),\n",
    "                               param_grid=nb_bernoulli_grid,\n",
    "                               cv=5,\n",
    "                               verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for Naive Bayes (BernoulliNB)\n",
    "gs_nb_bernoulli.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for Bernoulli Naive Bayes with GridSearchCV:\", gs_nb_bernoulli.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (Bernoulli Naive Bayes) model test score:\", gs_nb_bernoulli.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VotingClassifier with GridSearchCV\n",
    "\n",
    "# Setup grid hyperparameter search for VotingClassifier\n",
    "gs_voting = GridSearchCV(VotingClassifier(estimators=[('log_reg', log_reg), ('rf', rf), \n",
    "                                                    #   ('svc', svc),\n",
    "                                                      ('naive_bayes', naive_bayes), ('knn', knn)], voting='soft'),\n",
    "                         param_grid=voting_grid,\n",
    "                         cv=5,\n",
    "                         verbose=True)\n",
    "\n",
    "# Fit grid hyperparameter search for VotingClassifier\n",
    "gs_voting.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for VotingClassifier with GridSearchCV:\", gs_voting.best_params_)\n",
    "\n",
    "print(\"GridSearchCV (VotingClassifier) model test score:\", gs_voting.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate classification error\n",
    "def classification_error(y_true, y_pred):\n",
    "    return 1 - accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Define a function to calculate specificity\n",
    "def specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum(axis=1) - cm.diagonal()  # True negatives for each class\n",
    "    fp = cm.sum(axis=0) - cm.diagonal()  # False positives for each class\n",
    "    specificity_per_class = tn / (tn + fp + 1e-10)  # Avoid division by zero\n",
    "    return specificity_per_class.mean()  # Average specificity across all classes\n",
    "\n",
    "tuned_model_scores = {\n",
    "     \"KNN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_knn.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_knn.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_knn.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_knn.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_knn.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_log_reg.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_log_reg.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_log_reg.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_log_reg.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_rf.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_rf.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_rf.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, rs_rf.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, rs_dt.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, rs_dt.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, rs_dt.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, rs_dt.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (Linear)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_svc_linear.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_svc_linear.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_svc_linear.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_svc_linear.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_svc_linear.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_svc_linear.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (RBF)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_svc_rbf.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_svc_rbf.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_svc_rbf.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_svc_rbf.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_svc_rbf.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_svc_rbf.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (Poly)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_svc_poly.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_svc_poly.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_svc_poly.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_svc_poly.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_svc_poly.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_svc_poly.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"SVC (Sigmoid)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_svc_sigmoid.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_svc_sigmoid.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_svc_sigmoid.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_svc_sigmoid.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_svc_sigmoid.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_svc_sigmoid.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"ANN\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_ann.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_ann.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_ann.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_ann.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_ann.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_ann.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (GaussianNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_nb_gaussian.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_nb_gaussian.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_nb_gaussian.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_nb_gaussian.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"Naive Bayes (BernoulliNB)\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_nb_bernoulli.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_nb_bernoulli.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_nb_bernoulli.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_nb_bernoulli.predict(X_test), average=\"weighted\"),\n",
    "    },\n",
    "    \"VotingClassifier\": {\n",
    "        \"Accuracy\": accuracy_score(y_test, gs_voting.predict(X_test)),\n",
    "        \"Precision\": precision_score(y_test, gs_voting.predict(X_test), average=\"weighted\"),\n",
    "        \"Sensitivity (Recall)\": recall_score(y_test, gs_voting.predict(X_test), average=\"weighted\"),\n",
    "        \"Classification Error\": classification_error(y_test, gs_voting.predict(X_test)),\n",
    "        \"Specificity\": specificity(y_test, gs_voting.predict(X_test)),\n",
    "        \"F1 Measure\": f1_score(y_test, gs_voting.predict(X_test), average=\"weighted\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert results to a DataFrame and scale to percentages\n",
    "tuned_model_comparison = pd.DataFrame(tuned_model_scores).T * 100\n",
    "\n",
    "# Display the DataFrame\n",
    "print(tuned_model_comparison)\n",
    "\n",
    "# Optional: Pretty-print the table\n",
    "tuned_model_comparison.style.format({\n",
    "    \"Accuracy\": \"{:.2f}%\",\n",
    "    \"Precision\": \"{:.2f}%\",\n",
    "    \"Sensitivity (Recall)\": \"{:.2f}%\",\n",
    "    \"Classification Error\": \"{:.2f}%\",\n",
    "    \"Specificity\": \"{:.2f}%\",\n",
    "    \"F1 Measure\": \"{:.2f}%\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Tuning models with Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models with the best hyperparameters (from RandomizedSearchCV or GridSearchCV)\n",
    "\n",
    "# Define the best models with their respective best parameters\n",
    "best_models = {\n",
    "    \"KNN\": KNeighborsClassifier(**rs_knn.best_params_),\n",
    "    \"LogisticRegression\": LogisticRegression(**rs_log_reg.best_params_),\n",
    "    \"RandomForest\": RandomForestClassifier(**rs_rf.best_params_),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(**rs_dt.best_params_),\n",
    "    \"SVC_Linear\": SVC(**rs_svc_linear.best_params_),\n",
    "    \"SVC_RBF\": SVC(**rs_svc_rbf.best_params_),\n",
    "    \"SVC_Poly\": SVC(**rs_svc_poly.best_params_),\n",
    "    \"SVC_Sigmoid\": SVC(**rs_svc_sigmoid.best_params_),\n",
    "    \"ANN\": MLPClassifier(**rs_ann.best_params_),\n",
    "    \"NaiveBayes_Gaussian\": GaussianNB(**rs_nb_gaussian.best_params_),\n",
    "    \"NaiveBayes_Bernoulli\": BernoulliNB(**rs_nb_bernoulli.best_params_)\n",
    "}\n",
    "\n",
    "best_models[\"VotingClassifier\"] = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', best_models['KNN']),\n",
    "        ('log_reg', best_models['LogisticRegression']),\n",
    "        ('rf', best_models['RandomForest']),\n",
    "        ('naive_bayes', best_models['NaiveBayes_Gaussian'])\n",
    "    ],\n",
    "    **rs_voting.best_params_\n",
    ")\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring_metrics = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score, average='weighted'),\n",
    "    \"recall\": make_scorer(recall_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Cross-validation for each model and store results\n",
    "cv_results = {}\n",
    "for model_name, model in best_models.items():\n",
    "    scores = cross_validate(model, X_train, y_train, cv=10, scoring=scoring_metrics, verbose=True)\n",
    "    cv_results[model_name] = {\n",
    "        \"accuracy\": scores['test_accuracy'].mean() * 100,\n",
    "        \"precision\": scores['test_precision'].mean() * 100,\n",
    "        \"recall\": scores['test_recall'].mean() * 100\n",
    "    }\n",
    "\n",
    "# Create a summary DataFrame\n",
    "cv_results_summary = {\n",
    "    \"Model\": [],\n",
    "    \"Mean Accuracy (%)\": [],\n",
    "    \"Mean Precision (%)\": [],\n",
    "    \"Mean Recall (%)\": []\n",
    "}\n",
    "\n",
    "for model_name, metrics in cv_results.items():\n",
    "    cv_results_summary[\"Model\"].append(model_name)\n",
    "    cv_results_summary[\"Mean Accuracy (%)\"].append(metrics[\"accuracy\"])\n",
    "    cv_results_summary[\"Mean Precision (%)\"].append(metrics[\"precision\"])\n",
    "    cv_results_summary[\"Mean Recall (%)\"].append(metrics[\"recall\"])\n",
    "\n",
    "results_df = pd.DataFrame(cv_results_summary)\n",
    "\n",
    "# Style the DataFrame\n",
    "styled_table = results_df.style.format({\n",
    "    \"Mean Accuracy (%)\": \"{:.2f}%\",\n",
    "    \"Mean Precision (%)\": \"{:.2f}%\",\n",
    "    \"Mean Recall (%)\": \"{:.2f}%\"\n",
    "}).set_caption(\"Cross-Validation Results Summary\")\n",
    "\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make preidctions on test data\n",
    "y_preds = gs_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 ROC Curve and AUC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC score\n",
    "y_pred_prob = gs_log_reg.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
    "auc_score = roc_auc_score(y_test, y_pred_prob) # Calculate AUC score\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # Set a larger figure for better readability\n",
    "RocCurveDisplay.from_estimator(\n",
    "    estimator=gs_log_reg,\n",
    "    X=X_test,\n",
    "    y=y_test,\n",
    "    ax=ax,\n",
    "    name=\"Logistic Regression\"  # You can change the model name if necessary\n",
    ")\n",
    "\n",
    "# Enhance plot appearance\n",
    "ax.set_title(\"ROC Curve with AUC\", fontsize=16)\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=14)\n",
    "ax.set_ylabel(\"True Positive Rate\", fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.8)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Creating a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gs_log_reg.classes_)\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\", xticks_rotation=45)\n",
    "plt.grid(False)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\\n\")\n",
    "report = classification_report(y_test, y_preds, output_dict=True)\n",
    "report_df = pd.DataFrame(report).T\n",
    "display(report_df.style.format(subset=[\"precision\", \"recall\", \"f1-score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Cross-validated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clf using the best hyperparameters from GridSearchCV\n",
    "clf = LogisticRegression(C=gs_log_reg.best_params_['C'], solver=\"liblinear\")\n",
    "\n",
    "# Cross-validated metrics\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": np.mean(cross_val_score(clf, X, y, cv=5, scoring=\"accuracy\")),\n",
    "    \"Precision\": np.mean(cross_val_score(clf, X, y, cv=5, scoring=\"precision\")),\n",
    "    \"Recall\": np.mean(cross_val_score(clf, X, y, cv=5, scoring=\"recall\")),\n",
    "    \"F1\": np.mean(cross_val_score(clf, X, y, cv=5, scoring=\"f1\")),\n",
    "}\n",
    "\n",
    "# Convert metrics to DataFrame\n",
    "cv_metrics_df = pd.DataFrame(cv_metrics, index=[\"Score\"]).T\n",
    "cv_metrics_df.columns = [\"Cross-Validated\"]\n",
    "\n",
    "# Plot metrics\n",
    "cv_metrics_df.plot(kind=\"bar\", legend=False)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=90, ha='center')\n",
    "\n",
    "# Set y-axis limits and format y-axis to percentage\n",
    "plt.ylim(0, 1)\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{y*100:.0f}%'))\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an instance of LogisticRegression (taken from above)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Extract coefficients and match features to columns\n",
    "features_dict = dict(zip(df.columns, list(clf.coef_[0])))\n",
    "\n",
    "# Convert to DataFrame and sort by importance\n",
    "features_df = pd.DataFrame(features_dict.items(), columns=[\"Feature\", \"Importance\"])\n",
    "features_df = features_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Visualize feature importance (sorted)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=features_df, x=\"Importance\", y=\"Feature\", palette=\"viridis\")\n",
    "plt.title(\"Feature Importance using Logistic Regression\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
